{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50724607-a709-4635-98ca-018891a36377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import nbimporter\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from my_dataset import VinDrCXRDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16c7179b-31a8-4211-9ceb-86f912cfb2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA not available. Using CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a0d5c33-a98d-44dc-af48-633769a997c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a5f172c-28d8-4ce3-87ce-ed4e5586991e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1650'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a8409f9-c422-42c5-8e81-4d42ddd43b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac5d058",
   "metadata": {},
   "source": [
    "Swin Transformer Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f4e5386-84a5-46d9-8d50-0d0a0841c415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import nbimporter\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from my_dataset import VinDrCXRDataset\n",
    "import numpy as np\n",
    "from einops import rearrange, reduce, repeat\n",
    "import pydicom\n",
    "from pydicom.pixel_data_handlers.util import apply_voi_lut\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from torchvision.ops import misc as misc_nn_ops\n",
    "from torchvision.models.detection.backbone_utils import BackboneWithFPN\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.ops.feature_pyramid_network import FeaturePyramidNetwork, LastLevelMaxPool\n",
    "from torchvision.ops.misc import FrozenBatchNorm2d\n",
    "import torch.nn as nn\n",
    "from torch import einsum\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3baece50",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db0616cf-5ba6-4cf1-9ffc-aab4c8a3509f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclicShift(nn.Module):\n",
    "    def __init__(self, displacement):\n",
    "        super().__init__()\n",
    "        self.displacement = displacement\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.roll(x, shifts=(self.displacement, self.displacement), dims=(1, 2))\n",
    "\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(x, **kwargs) + x\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def create_mask(window_size, displacement, upper_lower, left_right):\n",
    "    mask = torch.zeros(window_size ** 2, window_size ** 2)\n",
    "\n",
    "    if upper_lower:\n",
    "        mask[-displacement * window_size:, :-displacement * window_size] = float('-inf')\n",
    "        mask[:-displacement * window_size, -displacement * window_size:] = float('-inf')\n",
    "\n",
    "    if left_right:\n",
    "        mask = rearrange(mask, '(h1 w1) (h2 w2) -> h1 w1 h2 w2', h1=window_size, h2=window_size)\n",
    "        mask[:, -displacement:, :, :-displacement] = float('-inf')\n",
    "        mask[:, :-displacement, :, -displacement:] = float('-inf')\n",
    "        mask = rearrange(mask, 'h1 w1 h2 w2 -> (h1 w1) (h2 w2)')\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_relative_distances(window_size):\n",
    "    indices = torch.tensor(np.array([[x, y] for x in range(window_size) for y in range(window_size)]))\n",
    "    distances = indices[None, :, :] - indices[:, None, :]\n",
    "    return distances\n",
    "\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    def __init__(self, dim, heads, head_dim, shifted, window_size, relative_pos_embedding):\n",
    "        super().__init__()\n",
    "        inner_dim = head_dim * heads\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "        self.window_size = window_size\n",
    "        self.relative_pos_embedding = relative_pos_embedding\n",
    "        self.shifted = shifted\n",
    "\n",
    "        if self.shifted:\n",
    "            displacement = window_size // 2\n",
    "            self.cyclic_shift = CyclicShift(-displacement)\n",
    "            self.cyclic_back_shift = CyclicShift(displacement)\n",
    "            self.upper_lower_mask = nn.Parameter(create_mask(window_size=window_size, displacement=displacement,\n",
    "                                                             upper_lower=True, left_right=False), requires_grad=False)\n",
    "            self.left_right_mask = nn.Parameter(create_mask(window_size=window_size, displacement=displacement,\n",
    "                                                            upper_lower=False, left_right=True), requires_grad=False)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "\n",
    "        if self.relative_pos_embedding:\n",
    "            self.relative_indices = get_relative_distances(window_size) + window_size - 1\n",
    "            self.pos_embedding = nn.Parameter(torch.randn(2 * window_size - 1, 2 * window_size - 1))\n",
    "        else:\n",
    "            self.pos_embedding = nn.Parameter(torch.randn(window_size ** 2, window_size ** 2))\n",
    "\n",
    "        self.to_out = nn.Linear(inner_dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.shifted:\n",
    "            x = self.cyclic_shift(x)\n",
    "\n",
    "        b, n_h, n_w, _, h = *x.shape, self.heads\n",
    "\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        nw_h = n_h // self.window_size\n",
    "        nw_w = n_w // self.window_size\n",
    "\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(t, 'b (nw_h w_h) (nw_w w_w) (h d) -> b h (nw_h nw_w) (w_h w_w) d',\n",
    "                                h=h, w_h=self.window_size, w_w=self.window_size), qkv)\n",
    "\n",
    "        dots = einsum('b h w i d, b h w j d -> b h w i j', q, k) * self.scale\n",
    "\n",
    "        if self.relative_pos_embedding:\n",
    "            dots += self.pos_embedding[self.relative_indices[:, :, 0], self.relative_indices[:, :, 1]]\n",
    "        else:\n",
    "            dots += self.pos_embedding\n",
    "\n",
    "        if self.shifted:\n",
    "            dots[:, :, -nw_w:] += self.upper_lower_mask\n",
    "            dots[:, :, nw_w - 1::nw_w] += self.left_right_mask\n",
    "\n",
    "        attn = dots.softmax(dim=-1)\n",
    "\n",
    "        out = einsum('b h w i j, b h w j d -> b h w i d', attn, v)\n",
    "        out = rearrange(out, 'b h (nw_h nw_w) (w_h w_w) d -> b (nw_h w_h) (nw_w w_w) (h d)',\n",
    "                        h=h, w_h=self.window_size, w_w=self.window_size, nw_h=nw_h, nw_w=nw_w)\n",
    "        out = self.to_out(out)\n",
    "\n",
    "        if self.shifted:\n",
    "            out = self.cyclic_back_shift(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class SwinBlock(nn.Module):\n",
    "    def __init__(self, dim, heads, head_dim, mlp_dim, shifted, window_size, relative_pos_embedding):\n",
    "        super().__init__()\n",
    "        self.attention_block = Residual(PreNorm(dim, WindowAttention(dim=dim,\n",
    "                                                                     heads=heads,\n",
    "                                                                     head_dim=head_dim,\n",
    "                                                                     shifted=shifted,\n",
    "                                                                     window_size=window_size,\n",
    "                                                                     relative_pos_embedding=relative_pos_embedding)))\n",
    "        self.mlp_block = Residual(PreNorm(dim, FeedForward(dim=dim, hidden_dim=mlp_dim)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attention_block(x)\n",
    "        x = self.mlp_block(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchMerging(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, downscaling_factor):\n",
    "        super().__init__()\n",
    "        self.downscaling_factor = downscaling_factor\n",
    "        self.patch_merge = nn.Unfold(kernel_size=downscaling_factor, stride=downscaling_factor, padding=0)\n",
    "        self.linear = nn.Linear(in_channels * downscaling_factor ** 2, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        new_h, new_w = h // self.downscaling_factor, w // self.downscaling_factor\n",
    "        x = self.patch_merge(x).view(b, -1, new_h, new_w).permute(0, 2, 3, 1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class StageModule(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dimension, layers, downscaling_factor, num_heads, head_dim, window_size,\n",
    "                 relative_pos_embedding):\n",
    "        super().__init__()\n",
    "        assert layers % 2 == 0, 'Stage layers need to be divisible by 2 for regular and shifted block.'\n",
    "\n",
    "        self.patch_partition = PatchMerging(in_channels=in_channels, out_channels=hidden_dimension,\n",
    "                                            downscaling_factor=downscaling_factor)\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(layers // 2):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                SwinBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dimension * 4,\n",
    "                          shifted=False, window_size=window_size, relative_pos_embedding=relative_pos_embedding),\n",
    "                SwinBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dimension * 4,\n",
    "                          shifted=True, window_size=window_size, relative_pos_embedding=relative_pos_embedding),\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_partition(x)\n",
    "        for regular_block, shifted_block in self.layers:\n",
    "            x = regular_block(x)\n",
    "            x = shifted_block(x)\n",
    "        return x.permute(0, 3, 1, 2)\n",
    "    \n",
    "    \n",
    "class SwinTransformer(nn.Module):\n",
    "    def __init__(self, *, hidden_dim=96, layers=(2, 2, 6, 2), heads=(3, 6, 12, 34), channels=3, num_classes=1000, head_dim=32, window_size=7,\n",
    "                 downscaling_factors=(4, 2, 2, 2), relative_pos_embedding=True):\n",
    "        super().__init__()\n",
    "\n",
    "        \n",
    "\n",
    "        # Your Swin Transformer stages remain the same\n",
    "        self.stage1 = StageModule(in_channels=channels, hidden_dimension=hidden_dim, layers=layers[0],\n",
    "                                  downscaling_factor=downscaling_factors[0], num_heads=heads[0], head_dim=head_dim,\n",
    "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
    "        self.stage2 = StageModule(in_channels=hidden_dim, hidden_dimension=hidden_dim * 2, layers=layers[1],\n",
    "                                  downscaling_factor=downscaling_factors[1], num_heads=heads[1], head_dim=head_dim,\n",
    "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
    "        self.stage3 = StageModule(in_channels=hidden_dim * 2, hidden_dimension=hidden_dim * 4, layers=layers[2],\n",
    "                                  downscaling_factor=downscaling_factors[2], num_heads=heads[2], head_dim=head_dim,\n",
    "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
    "        self.stage4 = StageModule(in_channels=hidden_dim * 4, hidden_dimension=hidden_dim * 8, layers=layers[3],\n",
    "                                  downscaling_factor=downscaling_factors[3], num_heads=heads[3], head_dim=head_dim,\n",
    "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
    "        \n",
    " #Example output channels of each stage, adjust based on your model specifics\n",
    "        self.in_channels_list = [hidden_dim * 2 ** i for i in range(4)]  # [96, 192, 384, 768]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through each stage\n",
    "        x1 = self.stage1(x)  \n",
    "        x2 = self.stage2(x1)  \n",
    "        x3 = self.stage3(x2)  \n",
    "        x4 = self.stage4(x3)  \n",
    "        \n",
    "        # Return a dict of tensors for FPN integration\n",
    "        return {'p2': x1, 'p3': x2, 'p4': x3, 'p5': x4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77d330d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fc876f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_swin_transformer_fpn_backbone(hidden_dim=96):\n",
    "    # Initialize your Swin Transformer Backbone\n",
    "    backbone = SwinTransformer(hidden_dim=hidden_dim)\n",
    "    \n",
    "    #specify which layers of the backbone to use for FPN\n",
    "    return_layers = {'p2': '0', 'p3' : '1', 'p4' : '2', 'p5' : '3'}\n",
    "\n",
    "    #backbone.norm = misc_nn_ops.FrozenBatchNorm2d(backbone.out_channels)\n",
    "    in_channels_list = backbone.in_channels_list\n",
    "    \n",
    "  # Feature Pyramid Network creatation and Integration\n",
    "    backbone_with_fpn = BackboneWithFPN(\n",
    "        backbone=backbone,\n",
    "        return_layers=return_layers,\n",
    "        in_channels_list=in_channels_list,\n",
    "        out_channels=256,\n",
    "        extra_blocks=LastLevelMaxPool(),\n",
    "\n",
    "    )\n",
    "   \n",
    "    return backbone_with_fpn\n",
    "\n",
    "\n",
    "# Create the backbone\n",
    "backbone_with_fpn = create_swin_transformer_fpn_backbone(hidden_dim=96)\n",
    "\n",
    "# Create the Faster R-CNN model using the Swin Transformer backbone\n",
    "model = FasterRCNN(backbone_with_fpn, num_classes=15, rpn_anchor_generator=AnchorGenerator()).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135ab0ee",
   "metadata": {},
   "source": [
    "Swin_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bc0050",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def swin_t(hidden_dim=96, layers=(2, 2, 6, 2), heads=(3, 6, 12, 24), **kwargs):\n",
    "    return SwinTransformer(hidden_dim=hidden_dim, layers=layers, heads=heads, **kwargs)\n",
    "\n",
    "\n",
    "# import torch\n",
    "# from swin_transformer_pytorch import SwinTransformer\n",
    "\n",
    "# net = SwinTransformer(\n",
    "#     hidden_dim=96,\n",
    "#     layers=(2, 2, 6, 2),\n",
    "#     heads=(3, 6, 12, 24),\n",
    "#     channels=3,\n",
    "#     num_classes=3,\n",
    "#     head_dim=32,\n",
    "#     window_size=7,\n",
    "#     downscaling_factors=(4, 2, 2, 2),\n",
    "#     relative_pos_embedding=True\n",
    "# )\n",
    "# dummy_x = torch.randn(1, 3, 224, 224)\n",
    "# logits = net(dummy_x)  # (1,3)\n",
    "# print(net)\n",
    "# print(logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dbb069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "def validate(model, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    with torch.no_grad():\n",
    "        for images, targets in val_loader:\n",
    "            images = list(img.to(device) for img in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            with autocast():\n",
    "                loss_dict = model(images, targets)\n",
    "                losses = sum(loss for loss in loss_dict.values())\n",
    "                \n",
    "            val_loss.append(losses.item())\n",
    "    \n",
    "    return np.mean(val_loss)\n",
    "\n",
    "# Initialize the gradient scaler for mixed-precision training\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Early Stopping Parameters\n",
    "num_epochs = 10\n",
    "best_val_loss = float('inf')\n",
    "patience = 3\n",
    "trigger_times = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    for images, targets in train_loader:\n",
    "        images = list(img.to(device) for img in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast():\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            \n",
    "        scaler.scale(losses).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        train_loss.append(losses.item())\n",
    "    \n",
    "    scheduler.step()\n",
    "    avg_train_loss = np.mean(train_loss)\n",
    "    avg_val_loss = validate(model, val_loader, device)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # Early Stopping Check based on validation loss improvement\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        trigger_times = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")  # Save best model\n",
    "        print(\"Model saved as validation loss improved.\")\n",
    "    else:\n",
    "        trigger_times += 1\n",
    "        if trigger_times >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}. No improvement in validation loss for {patience} consecutive epochs.\")\n",
    "            break\n",
    "\n",
    "print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e633c6c7",
   "metadata": {},
   "source": [
    "Important Notes and Adjustments:\n",
    "Stages Output: Adjust the forward method in SwinTransformerBackbone to output feature maps from multiple stages if you plan to utilize an FPN effectively. FPN requires features at multiple scales to improve detection across different object sizes.\n",
    "\n",
    "Normalization Layer: If your Swin Transformer model requires specific normalization (e.g., LayerNorm), you might need to adjust the implementation to ensure it's compatible with Faster R-CNN's expectations. The example uses FrozenBatchNorm2d for simplicity and efficiency.\n",
    "\n",
    "FPN Configuration: The BackboneWithFPN is configured with a simplified setup. In practice, you should adjust return_layers, in_channels_list, and out_channels based on the actual sizes of feature maps output by your Swin Transformer stages.\n",
    "\n",
    "Hyperparameters: Be prepared to tune hyperparameters extensively. The integration of a transformer model with Faster R-CNN, especially regarding learning rate, weight decay, and training schedule, might require adjustments for optimal performance.\n",
    "\n",
    "Pretraining: If you have a pretrained Swin Transformer model, ensure to load the weights correctly before wrapping it with FPN and passing it to Faster R-CNN. Pretrained weights can significantly boost performance, especially in complex datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255fb178",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
